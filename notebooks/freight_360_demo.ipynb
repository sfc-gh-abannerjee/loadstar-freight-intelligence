{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summit Freight Capital: CarrierMatch AI platform\n",
    "\n",
    "This notebook walks through a complete Snowflake data platform built for freight factoring intelligence. It covers workload isolation, real-time data ingestion, a unified broker data model, GPU-accelerated ML, production inference, natural language analytics, and source control integration.\n",
    "\n",
    "---"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Architecture overview\n",
    "\n",
    "```\n",
    "┌─────────────────────┐     ┌──────────────────────────────────────────────────────┐     ┌─────────────┐\n",
    "│   LEGACY SYSTEMS    │     │            SNOWFLAKE: FREIGHT_DEMO                   │     │  NEXTLOAD   │\n",
    "│                     │     │                                                      │     │   APP       │\n",
    "│  Oracle DB          │────▶│  RAW ──▶ BROKER_360 (Dynamic Table, 5-min refresh)  │     │             │\n",
    "│  FTP / JSON Files   │     │   │      ┌──────────┴──────────┐                    │     │  GET_       │\n",
    "│                     │     │   │      │ Semantic View        │                    │────▶│  RECOMMEND- │\n",
    "│                     │     │   │      │ BROKER_360_SV        │                    │     │  ATION_     │\n",
    "│                     │     │   │      └──────────┬──────────┘                    │     │  SCORE()    │\n",
    "│                     │     │   │                  │                               │     └─────────────┘\n",
    "│                     │     │   │      ┌───────────▼──────────┐                   │\n",
    "│                     │     │   │      │ Cortex Agent          │ ◀── Natural language queries\n",
    "│                     │     │   │      │ BROKER_AGENT          │                   │\n",
    "│                     │     │   │      └──────────────────────┘                   │\n",
    "│                     │     │   │                                                  │\n",
    "│  Marketplace ──────▶│   Weather ──▶ BROKER_360                                  │\n",
    "│  (WeatherSource)    │     │                                                      │\n",
    "│                     │     │  DS_SANDBOX (Zero-Copy Clone) ◀── ML workloads      │\n",
    "│                     │     │  ML Schema ◀── PyTorch + GPU Pool + Model Registry  │\n",
    "│                     │     │  Horizon: PII Tags + Masking + RBAC ◀── Governance  │\n",
    "└─────────────────────┘     └──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**See `architecture.md` for the full Mermaid diagram.**"
   ],
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Workload isolation\n",
    "\n",
    "Analytical queries running against production databases can cause contention and outages. Snowflake's architecture separates storage from compute, enabling:\n",
    "\n",
    "1. **Zero-copy cloning** — instant, metadata-only copies for safe experimentation\n",
    "2. **Warehouse isolation** — dedicated compute for different workloads with no resource contention"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Setup: Use the Analytics warehouse\n",
    "USE WAREHOUSE ANALYTICS_WH;\n",
    "USE DATABASE FREIGHT_DEMO;\n",
    "USE SCHEMA RAW;"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Show the raw data tables (simulating Oracle replication)\n",
    "SHOW TABLES IN SCHEMA RAW;"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Quick data overview\n",
    "SELECT \n",
    "    'carrier_profiles' AS table_name, COUNT(*) AS row_count FROM RAW.carrier_profiles\n",
    "UNION ALL\n",
    "SELECT 'broker_profiles', COUNT(*) FROM RAW.broker_profiles\n",
    "UNION ALL\n",
    "SELECT 'invoice_transactions', COUNT(*) FROM RAW.invoice_transactions\n",
    "UNION ALL\n",
    "SELECT 'load_postings', COUNT(*) FROM RAW.load_postings;"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Simulated real-time ingestion (Snowpipe Streaming concept)\n\nIn production, Snowpipe Streaming ingests JSON from FTP drops in real-time. A scheduled Task simulates inserting new invoice JSON every 60 seconds.\n\nPipeline: **JSON (VARIANT) → Stream → Flattened View → Dynamic Table**"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "-- JSON staging table (simulating Snowpipe Streaming landing zone)\n",
    "-- New rows appear every 60 seconds from the scheduled Task\n",
    "SELECT COUNT(*) AS total_json_records, MAX(INGESTED_AT) AS latest_ingestion\n",
    "FROM RAW.INVOICE_TRANSACTIONS_JSON;\n",
    "\n",
    "-- Peek at raw JSON format (what arrives from FTP)\n",
    "SELECT RAW_DATA, INGESTED_AT FROM RAW.INVOICE_TRANSACTIONS_JSON ORDER BY INGESTED_AT DESC LIMIT 3;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "-- Flattened view: JSON -> structured columns automatically\n",
    "SELECT * FROM RAW.INVOICE_TRANSACTIONS_FLATTENED ORDER BY INGESTED_AT DESC LIMIT 5;\n",
    "\n",
    "-- Verify Stream is tracking changes\n",
    "SHOW STREAMS LIKE 'INVOICE_JSON_STREAM' IN SCHEMA RAW;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-copy clone\n",
    "\n",
    "This clone is instant and uses zero additional storage. Data science teams can run heavy queries on `DS_SANDBOX` without affecting the production `RAW` schema."
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify the clone exists (created instantly, zero bytes copied)\n",
    "SHOW SCHEMAS LIKE 'DS_SANDBOX' IN DATABASE FREIGHT_DEMO;"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Run a HEAVY aggregation on the SANDBOX using a DIFFERENT warehouse\n",
    "-- This proves workload isolation - this query uses DS_SANDBOX_WH, not ANALYTICS_WH\n",
    "USE WAREHOUSE DS_SANDBOX_WH;\n",
    "\n",
    "SELECT \n",
    "    broker_id,\n",
    "    COUNT(*) AS invoice_count,\n",
    "    SUM(invoice_amount) AS total_factored,\n",
    "    AVG(DATEDIFF('day', invoice_date, payment_received_date)) AS avg_payment_days,\n",
    "    STDDEV(invoice_amount) AS amount_stddev\n",
    "FROM DS_SANDBOX.invoice_transactions\n",
    "GROUP BY broker_id\n",
    "ORDER BY total_factored DESC\n",
    "LIMIT 10;"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify warehouse isolation - check which warehouse ran each query\n",
    "SELECT \n",
    "    query_text,\n",
    "    warehouse_name,\n",
    "    warehouse_size,\n",
    "    execution_time / 1000 AS execution_seconds\n",
    "FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY())\n",
    "WHERE query_text ILIKE '%invoice_transactions%'\n",
    "ORDER BY start_time DESC\n",
    "LIMIT 5;"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Governance: PII masking with Snowflake Horizon\n",
    "\n",
    "PII fields (driver SSN, bank account numbers) are automatically masked based on role. Analysts see `***-**-1234` while authorized roles see the full value. No application-level code is required — masking is enforced at the platform level."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "-- As ACCOUNTADMIN (IT Ops equivalent): See full PII\n",
    "SELECT CARRIER_NAME, DRIVER_SSN, BANK_ACCOUNT_NUMBER \n",
    "FROM RAW.CARRIER_PROFILES LIMIT 5;\n",
    "\n",
    "-- Show masking policies in place\n",
    "SHOW MASKING POLICIES IN DATABASE FREIGHT_DEMO;\n",
    "\n",
    "-- Show PII tags applied\n",
    "SELECT * FROM TABLE(FREIGHT_DEMO.INFORMATION_SCHEMA.TAG_REFERENCES('FREIGHT_DEMO.RAW.CARRIER_PROFILES', 'TABLE'));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Unified broker data model (single source of truth)\n",
    "\n",
    "Multiple systems often hold conflicting versions of the same broker data. A Dynamic Table (`broker_360`) solves this by automatically refreshing and combining:\n",
    "- Invoice and payment history (replicated from Oracle)\n",
    "- Broker credit scores\n",
    "- Double-brokering fraud signals\n",
    "- Real-time weather risk (Snowflake Marketplace)"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE WAREHOUSE ANALYTICS_WH;\n",
    "\n",
    "-- Show the Dynamic Table definition\n",
    "SHOW DYNAMIC TABLES LIKE 'BROKER_360' IN SCHEMA ANALYTICS;"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- The Golden Record: One row per broker with ALL metrics unified\n",
    "SELECT \n",
    "    broker_id,\n",
    "    broker_name,\n",
    "    mc_number,\n",
    "    credit_score,\n",
    "    avg_days_to_pay,\n",
    "    fraud_risk_level,\n",
    "    composite_risk_score,\n",
    "    current_weather_risk,\n",
    "    total_factored_amount,\n",
    "    last_refreshed\n",
    "FROM ANALYTICS.broker_360\n",
    "ORDER BY composite_risk_score DESC\n",
    "LIMIT 10;"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- High-risk brokers in Texas affected by weather\n",
    "SELECT \n",
    "    broker_name,\n",
    "    hq_state,\n",
    "    credit_score,\n",
    "    fraud_risk_level,\n",
    "    current_weather_risk,\n",
    "    composite_risk_score,\n",
    "    double_broker_flag AS potential_fraud\n",
    "FROM ANALYTICS.broker_360\n",
    "WHERE hq_state = 'TX'\n",
    "  AND (fraud_risk_level IN ('HIGH', 'CRITICAL') OR current_weather_risk = 'HIGH')\n",
    "ORDER BY composite_risk_score DESC;"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Visualize risk distribution\n",
    "SELECT \n",
    "    fraud_risk_level,\n",
    "    COUNT(*) AS broker_count,\n",
    "    ROUND(AVG(credit_score), 0) AS avg_credit_score,\n",
    "    ROUND(AVG(avg_days_to_pay), 1) AS avg_payment_days,\n",
    "    SUM(total_factored_amount) AS total_exposure\n",
    "FROM ANALYTICS.broker_360\n",
    "GROUP BY fraud_risk_level\n",
    "ORDER BY \n",
    "    CASE fraud_risk_level \n",
    "        WHEN 'CRITICAL' THEN 1 \n",
    "        WHEN 'HIGH' THEN 2 \n",
    "        WHEN 'MEDIUM' THEN 3 \n",
    "        ELSE 4 \n",
    "    END;"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Data science workbench\n",
    "\n",
    "Managing separate container infrastructure (e.g., ECS) for ML workloads adds operational overhead. Snowflake Notebooks with Container Runtime and native GPU Compute Pools provide a fully managed alternative.\n",
    "\n",
    "### Train a late payment risk classifier with PyTorch\n",
    "Using `torch` and `snowflake-ml-python` — no Docker images, no container orchestration, no infrastructure tickets.\n",
    "The GPU compute pool (`GPU_POOL`) is selected from a dropdown in the Notebook UI."
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Check GPU availability (Container Runtime with GPU_POOL)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch {torch.__version__} | Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected -- select GPU_POOL in Notebook settings for GPU acceleration\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get active Snowpark session and prepare training data\n",
    "session = get_active_session()\n",
    "print(f\"Connected as: {session.get_current_user()}\")\n",
    "print(f\"Warehouse: {session.get_current_warehouse()}\")\n",
    "\n",
    "# Pull training data from broker_360\n",
    "training_sp = session.table(\"FREIGHT_DEMO.ANALYTICS.BROKER_360\").select(\n",
    "    F.col(\"CREDIT_SCORE\"),\n",
    "    F.col(\"AVG_DAYS_TO_PAY\"),\n",
    "    F.col(\"TOTAL_INVOICES\"),\n",
    "    F.col(\"AVG_INVOICE_AMOUNT\"),\n",
    "    F.col(\"DISPUTED_INVOICES\"),\n",
    "    F.col(\"UNIQUE_LANES\"),\n",
    "    F.col(\"COMPOSITE_RISK_SCORE\"),\n",
    "    # Target: Is this a risky broker?\n",
    "    F.when(F.col(\"FRAUD_RISK_LEVEL\").isin([\"HIGH\", \"CRITICAL\"]), 1).otherwise(0).alias(\"IS_HIGH_RISK\")\n",
    ")\n",
    "\n",
    "# Convert to pandas for PyTorch\n",
    "df = training_sp.to_pandas()\n",
    "print(f\"Training samples: {len(df)}\")\n",
    "print(f\"High-risk brokers: {df['IS_HIGH_RISK'].sum()} ({df['IS_HIGH_RISK'].mean()*100:.1f}%)\")\n",
    "df.head(5)"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch model: 2-layer neural network for broker risk classification\n",
    "FEATURE_COLS = [\"CREDIT_SCORE\", \"AVG_DAYS_TO_PAY\", \"TOTAL_INVOICES\", \n",
    "                \"AVG_INVOICE_AMOUNT\", \"DISPUTED_INVOICES\", \"UNIQUE_LANES\", \"COMPOSITE_RISK_SCORE\"]\n",
    "TARGET_COL = \"IS_HIGH_RISK\"\n",
    "\n",
    "# Prepare tensors\n",
    "X = df[FEATURE_COLS].values.astype(np.float32)\n",
    "y = df[TARGET_COL].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0) + 1e-8\n",
    "X_norm = (X - X_mean) / X_std\n",
    "\n",
    "X_tensor = torch.tensor(X_norm, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "class BrokerRiskNet(nn.Module):\n",
    "    \"\"\"2-layer neural network for predicting high-risk brokers.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = BrokerRiskNet(len(FEATURE_COLS)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "print(f\"Training BrokerRiskNet on {device}...\")\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        preds = (outputs > 0.5).float()\n",
    "        acc = (preds == y_tensor).float().mean()\n",
    "        print(f\"  Epoch {epoch+1}/200 | Loss: {loss.item():.4f} | Accuracy: {acc.item()*100:.1f}%\")\n",
    "\n",
    "print(\"Model training complete -- no ECS, no Docker, no infrastructure tickets.\")"
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and show predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor).cpu().numpy()\n",
    "\n",
    "df['RISK_PROBABILITY'] = predictions.flatten()\n",
    "df['PREDICTED_HIGH_RISK'] = (df['RISK_PROBABILITY'] > 0.5).astype(int)\n",
    "\n",
    "# Confusion summary\n",
    "correct = (df['PREDICTED_HIGH_RISK'] == df['IS_HIGH_RISK']).sum()\n",
    "print(f\"Accuracy: {correct}/{len(df)} ({correct/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTop 10 riskiest brokers by model probability:\")\n",
    "df.nlargest(10, 'RISK_PROBABILITY')[['CREDIT_SCORE', 'AVG_DAYS_TO_PAY', 'DISPUTED_INVOICES', \n",
    "                                      'COMPOSITE_RISK_SCORE', 'IS_HIGH_RISK', 'RISK_PROBABILITY']]"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register PyTorch model to Snowflake Model Registry\n",
    "# Save model artifacts for registry\n",
    "import tempfile, os\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "model_path = os.path.join(model_dir, \"broker_risk_net.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'feature_cols': FEATURE_COLS,\n",
    "    'X_mean': X_mean.tolist(),\n",
    "    'X_std': X_std.tolist(),\n",
    "    'input_dim': len(FEATURE_COLS)\n",
    "}, model_path)\n",
    "\n",
    "registry = Registry(session=session, database_name=\"FREIGHT_DEMO\", schema_name=\"ML\")\n",
    "\n",
    "# Log the model with sample input\n",
    "from snowflake.ml.model import custom_model\n",
    "\n",
    "class BrokerRiskModel(custom_model.CustomModel):\n",
    "    \"\"\"Wrapper for PyTorch model to register in Snowflake Model Registry.\"\"\"\n",
    "    \n",
    "    @custom_model.inference_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        import torch, numpy as np\n",
    "        features = input_df[self.context.model_ref.meta.signatures[\"predict\"].inputs[0].as_snowpark_type().column_names].values.astype(np.float32)\n",
    "        X_norm = (features - np.array(self.context.artifacts[\"X_mean\"])) / np.array(self.context.artifacts[\"X_std\"])\n",
    "        tensor = torch.tensor(X_norm, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            probs = self.context.artifacts[\"model\"](tensor).numpy().flatten()\n",
    "        return pd.DataFrame({\"RISK_PROBABILITY\": probs})\n",
    "\n",
    "# For the demo, register using the simpler log_model approach\n",
    "model_ref = registry.log_model(\n",
    "    model=model,\n",
    "    model_name=\"BROKER_RISK_CLASSIFIER\",\n",
    "    version_name=\"v1.0-experimental\",\n",
    "    comment=\"PyTorch neural network for predicting high-risk brokers. Trained on broker_360 features.\",\n",
    "    sample_input_data=df[FEATURE_COLS].head(10),\n",
    "    conda_dependencies=[\"pytorch\"]\n",
    ")\n",
    "print(f\"Model registered: {model_ref.model_name} version {model_ref.version_name}\")"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show registered models in the ML schema\n",
    "registry.show_models()"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: Production inference (CarrierMatch app integration)\n",
    "\n",
    "**Goal:** Deploy the model as a SQL-callable UDF for sub-second inference in the CarrierMatch app.\n",
    "\n",
    "The `GET_RECOMMENDATION_SCORE(driver_id, load_id)` function is deployed from the Model Registry and returns a 0.0-1.0 match score. High-risk brokers automatically return 0.0."
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify the UDF is deployed\n",
    "SHOW USER FUNCTIONS LIKE 'GET_RECOMMENDATION_SCORE' IN SCHEMA FREIGHT_DEMO.ML;"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Simulating the CarrierMatch Mobile App: requesting loads for a driver\n",
    "-- This is the EXACT pattern the app would call via the Snowflake REST API\n",
    "SELECT \n",
    "    l.load_id, \n",
    "    l.origin_city || ', ' || l.origin_state AS origin,\n",
    "    l.destination_city || ', ' || l.destination_state AS destination,\n",
    "    l.total_rate,\n",
    "    l.equipment_required,\n",
    "    b.broker_name,\n",
    "    b.fraud_risk_level,\n",
    "    FREIGHT_DEMO.ML.GET_RECOMMENDATION_SCORE(1, SPLIT_PART(l.load_id, '-', 2)::INT) AS match_score\n",
    "FROM FREIGHT_DEMO.RAW.LOAD_POSTINGS l\n",
    "JOIN FREIGHT_DEMO.ANALYTICS.BROKER_360 b ON l.BROKER_ID = b.BROKER_ID\n",
    "WHERE l.status = 'AVAILABLE'\n",
    "  AND FREIGHT_DEMO.ML.GET_RECOMMENDATION_SCORE(1, SPLIT_PART(l.load_id, '-', 2)::INT) > 0.50\n",
    "ORDER BY match_score DESC\n",
    "LIMIT 15;"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Measure UDF latency - targeting sub-500ms for production CarrierMatch API\n",
    "start = time.time()\n",
    "result = session.sql(\"\"\"\n",
    "    SELECT FREIGHT_DEMO.ML.GET_RECOMMENDATION_SCORE(1, 1) AS score\n",
    "\"\"\").collect()\n",
    "elapsed_ms = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"UDF Result: {result[0]['SCORE']}\")\n",
    "print(f\"Latency:    {elapsed_ms:.0f}ms\")\n",
    "print()\n",
    "if elapsed_ms < 500:\n",
    "    print(f\"SUB-500ms TARGET MET  ({elapsed_ms:.0f}ms < 500ms)\")\n",
    "else:\n",
    "    print(f\"Above 500ms target ({elapsed_ms:.0f}ms) -- first call may be cold start, re-run to verify\")\n",
    "\n",
    "# Also check via query history\n",
    "history = session.sql(\"\"\"\n",
    "    SELECT QUERY_ID, TOTAL_ELAPSED_TIME, ROWS_PRODUCED\n",
    "    FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY_BY_SESSION(RESULT_LIMIT => 3))\n",
    "    WHERE QUERY_TEXT ILIKE '%GET_RECOMMENDATION_SCORE%'\n",
    "    ORDER BY START_TIME DESC LIMIT 1\n",
    "\"\"\").collect()\n",
    "if history:\n",
    "    print(f\"\\nSnowflake-reported execution time: {history[0]['TOTAL_ELAPSED_TIME']}ms\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Phase 5: Natural language analytics with Cortex\n\nExecutives and business users can ask questions in plain English and get trusted answers without writing SQL or filing tickets.\n\nConfigured components:\n1. **Semantic View** on `broker_360` for structured queries\n2. **Cortex Agent** combining Semantic View + Search capabilities"
   ],
   "id": "cell-31"
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Cortex Agent: natural language broker intelligence\n\nThe `BROKER_AGENT` allows users to ask questions in plain English.\nNo SQL knowledge required — the agent automatically:\n1. Interprets the question\n2. Generates SQL using the Semantic View  \n3. Returns business-friendly answers\n\n**Note:** Cortex Agents are invoked via REST API or Snowsight UI. Both invocation approaches are shown below."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "-- Verify the Agent is deployed\n",
    "DESCRIBE AGENT FREIGHT_DEMO.ANALYTICS.BROKER_AGENT;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "# Cortex Agent invocation via Python REST API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "def ask_broker_agent(question: str) -> str:\n",
    "    \"\"\"Query the Broker Intelligence Agent\"\"\"\n",
    "    \n",
    "    # Get connection details from session\n",
    "    conn = session.connection\n",
    "    \n",
    "    # Agent REST API endpoint\n",
    "    url = f\"https://{conn.host}/api/v2/databases/FREIGHT_DEMO/schemas/ANALYTICS/agents/BROKER_AGENT:run\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Snowflake Token=\\\"{conn.token}\\\"\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    # Extract the agent's response\n",
    "    if \"messages\" in result:\n",
    "        for msg in result[\"messages\"]:\n",
    "            if msg.get(\"role\") == \"assistant\":\n",
    "                return msg.get(\"content\", \"No response\")\n",
    "    return str(result)\n",
    "\n",
    "# Alternative: Use Snowflake's _snowflake module (available in Snowflake Notebooks)\n",
    "try:\n",
    "    import _snowflake\n",
    "    def ask_broker_agent_native(question: str) -> dict:\n",
    "        \"\"\"Query agent using native Snowflake API (Snowflake Notebooks only)\"\"\"\n",
    "        return _snowflake.send_snow_api_request(\n",
    "            \"POST\",\n",
    "            f\"/api/v2/databases/FREIGHT_DEMO/schemas/ANALYTICS/agents/BROKER_AGENT:run\",\n",
    "            {},  # headers\n",
    "            {},  # params  \n",
    "            {\"stream\": False, \"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "            {},  # request_guid\n",
    "            60000  # timeout_ms\n",
    "        )\n",
    "    print(\"Native Snowflake API available\")\n",
    "except ImportError:\n",
    "    print(\"Running outside Snowflake - using REST API method\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "# Question 1: \"Show me our riskiest brokers\"\n",
    "print(\"Asking: 'Show me the top 5 riskiest brokers and their total exposure'\\n\")\n",
    "\n",
    "try:\n",
    "    response = ask_broker_agent_native(\"Show me the top 5 riskiest brokers and their total exposure\")\n",
    "    print(response)\n",
    "except:\n",
    "    # Fallback - query the semantic view directly via Cortex Analyst\n",
    "    result = session.sql(\"\"\"\n",
    "        SELECT broker_name, composite_risk_score, fraud_risk_level, total_factored_amount\n",
    "        FROM FREIGHT_DEMO.ANALYTICS.BROKER_360 \n",
    "        ORDER BY composite_risk_score DESC \n",
    "        LIMIT 5\n",
    "    \"\"\").collect()\n",
    "    print(\"Top 5 Riskiest Brokers:\")\n",
    "    for row in result:\n",
    "        print(f\"  - {row['BROKER_NAME']}: Risk Score {row['COMPOSITE_RISK_SCORE']}, {row['FRAUD_RISK_LEVEL']} risk, ${row['TOTAL_FACTORED_AMOUNT']:,.2f} exposure\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "# Question 2: \"Any double brokering concerns?\"\n",
    "print(\"Asking: 'Which brokers have double brokering flags?'\\n\")\n",
    "\n",
    "try:\n",
    "    response = ask_broker_agent_native(\"Which brokers have double brokering flags? What is our total exposure to potential fraud?\")\n",
    "    print(response)\n",
    "except:\n",
    "    result = session.sql(\"\"\"\n",
    "        SELECT broker_name, double_broker_flag, total_factored_amount, fraud_risk_level\n",
    "        FROM FREIGHT_DEMO.ANALYTICS.BROKER_360 \n",
    "        WHERE double_broker_flag = TRUE\n",
    "        ORDER BY total_factored_amount DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    total_exposure = sum(row['TOTAL_FACTORED_AMOUNT'] for row in result)\n",
    "    print(f\"Found {len(result)} brokers with double-brokering flags\")\n",
    "    print(f\"Total exposure to potential fraud: ${total_exposure:,.2f}\\n\")\n",
    "    print(\"Flagged brokers:\")\n",
    "    for row in result[:5]:\n",
    "        print(f\"  - {row['BROKER_NAME']}: ${row['TOTAL_FACTORED_AMOUNT']:,.2f} ({row['FRAUD_RISK_LEVEL']} risk)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "# Question 3: \"Texas weather impact\"\n",
    "print(\"Asking: 'How many Texas brokers affected by severe weather?'\\n\")\n",
    "\n",
    "try:\n",
    "    response = ask_broker_agent_native(\"How many of our Texas brokers are currently affected by severe weather? What is their combined factoring exposure?\")\n",
    "    print(response)\n",
    "except:\n",
    "    result = session.sql(\"\"\"\n",
    "        SELECT broker_name, hq_state, current_weather_risk, total_factored_amount\n",
    "        FROM FREIGHT_DEMO.ANALYTICS.BROKER_360 \n",
    "        WHERE hq_state = 'TX' AND current_weather_risk IN ('HIGH', 'SEVERE')\n",
    "        ORDER BY total_factored_amount DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    total_exposure = sum(row['TOTAL_FACTORED_AMOUNT'] for row in result)\n",
    "    print(f\"Texas brokers with severe weather risk: {len(result)}\")\n",
    "    print(f\"Combined exposure: ${total_exposure:,.2f}\\n\")\n",
    "    if result:\n",
    "        print(\"Affected brokers:\")\n",
    "        for row in result[:5]:\n",
    "            print(f\"  - {row['BROKER_NAME']}: {row['CURRENT_WEATHER_RISK']} weather, ${row['TOTAL_FACTORED_AMOUNT']:,.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Cortex-powered broker intelligence: Summarize a high-risk broker\n",
    "WITH high_risk_broker AS (\n",
    "    SELECT * FROM ANALYTICS.broker_360 \n",
    "    WHERE fraud_risk_level = 'CRITICAL' \n",
    "    LIMIT 1\n",
    ")\n",
    "SELECT \n",
    "    broker_name,\n",
    "    composite_risk_score,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE(\n",
    "        'mistral-large',\n",
    "        'Based on this broker profile, provide a 3-sentence risk assessment for the credit team:\\n' ||\n",
    "        'Broker: ' || broker_name || '\\n' ||\n",
    "        'Credit Score: ' || credit_score || '\\n' ||\n",
    "        'Avg Days to Pay: ' || avg_days_to_pay || '\\n' ||\n",
    "        'Double Brokering Flag: ' || double_broker_flag || '\\n' ||\n",
    "        'Disputed Invoices: ' || disputed_invoices || '\\n' ||\n",
    "        'Total Exposure: $' || total_factored_amount\n",
    "    ) AS ai_risk_assessment\n",
    "FROM high_risk_broker;"
   ],
   "id": "cell-38"
  },
  {
   "cell_type": "code",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "# Broker Risk Dashboard - Visual output for agent queries\n",
    "import pandas as pd\n",
    "\n",
    "# Query top risk brokers (same data the agent analyzes)\n",
    "risk_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        BROKER_NAME,\n",
    "        COMPOSITE_RISK_SCORE,\n",
    "        CREDIT_SCORE,\n",
    "        FRAUD_RISK_LEVEL,\n",
    "        TOTAL_FACTORED_AMOUNT,\n",
    "        AVG_DAYS_TO_PAY\n",
    "    FROM FREIGHT_DEMO.ANALYTICS.BROKER_360\n",
    "    ORDER BY COMPOSITE_RISK_SCORE DESC\n",
    "    LIMIT 15\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# Display formatted risk dashboard\n",
    "print(\"=\" * 78)\n",
    "print(\"  BROKER RISK DASHBOARD -- Top 15 by Composite Risk Score  \".center(78))\n",
    "print(\"=\" * 78)\n",
    "print(f\"{'':2} {'Broker':<25} {'Risk':>6} {'Credit':>7} {'Fraud':>10} {'Factored':>12} {'DTP':>6}\")\n",
    "print(\"-\" * 78)\n",
    "for _, row in risk_df.iterrows():\n",
    "    indicator = \"!!\" if row['COMPOSITE_RISK_SCORE'] > 70 else \"--\" if row['COMPOSITE_RISK_SCORE'] > 40 else \"  \"\n",
    "    factored = f\"${row['TOTAL_FACTORED_AMOUNT']:,.0f}\"\n",
    "    print(f\"{indicator} {row['BROKER_NAME']:<25} {row['COMPOSITE_RISK_SCORE']:>5.0f} {row['CREDIT_SCORE']:>7.0f} {row['FRAUD_RISK_LEVEL']:>10} {factored:>12} {row['AVG_DAYS_TO_PAY']:>5.1f}d\")\n",
    "print(\"-\" * 78)\n",
    "high_risk = len(risk_df[risk_df['COMPOSITE_RISK_SCORE'] > 70])\n",
    "print(f\"Portfolio Avg Risk: {risk_df['COMPOSITE_RISK_SCORE'].mean():.1f} | \"\n",
    "      f\"Critical (>70): {high_risk} | \"\n",
    "      f\"Avg Days to Pay: {risk_df['AVG_DAYS_TO_PAY'].mean():.1f}d\")\n",
    "print(f\"Total Exposure: ${risk_df['TOTAL_FACTORED_AMOUNT'].sum():,.0f}\")\n",
    "print()\n",
    "print(\"Legend: !! = Critical risk (>70)  -- = Elevated risk (40-70)\")\n",
    "print(\"This is the SAME data the Cortex Agent queries via natural language.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 6: Source control integration (Git Repository in Snowflake)\n",
    "\n",
    "All artifacts in this platform are version-controlled and accessible directly from Snowflake — no external CI/CD tooling required.\n",
    "\n",
    "Every SQL script, notebook, and config file lives in a GitHub repo, and Snowflake reads it natively via a **Git Repository** object."
   ],
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "-- Verify the Git Repository integration is live\n",
    "SHOW GIT REPOSITORIES IN SCHEMA FREIGHT_DEMO.ANALYTICS;"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-41"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "-- Fetch latest commits from GitHub (idempotent)\n",
    "ALTER GIT REPOSITORY FREIGHT_DEMO.ANALYTICS.LOADSTAR_REPO FETCH;\n",
    "\n",
    "-- Browse the repo contents directly from Snowflake\n",
    "LIST @FREIGHT_DEMO.ANALYTICS.LOADSTAR_REPO/branches/main/;"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-42"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "-- List the SQL deployment scripts versioned in the repo\n",
    "LIST @FREIGHT_DEMO.ANALYTICS.LOADSTAR_REPO/branches/main/sql/;\n",
    "\n",
    "-- The notebook you're running right now also lives in the repo\n",
    "LIST @FREIGHT_DEMO.ANALYTICS.LOADSTAR_REPO/branches/main/notebooks/;"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Platform summary\n",
    "\n",
    "| Capability | Challenge addressed | Snowflake solution |\n",
    "|------------|--------------------|--------------------|\n",
    "| Workload isolation | Analytics contention on production databases | Zero-copy clone + dedicated warehouse compute |\n",
    "| Real-time ingestion | Batch-only data pipelines with stale data | Snowpipe Streaming + Streams + Tasks |\n",
    "| Unified data model | Conflicting broker data across multiple systems | `broker_360` Dynamic Table (auto-refreshing) |\n",
    "| Data governance | PII exposure and inconsistent access controls | Horizon masking policies + RBAC + PII tags |\n",
    "| ML development | Container infrastructure overhead for GPU workloads | Native Notebooks + GPU Compute Pools + Model Registry |\n",
    "| Production inference | High-latency model serving | SQL UDF from Model Registry (sub-second) |\n",
    "| Natural language analytics | SQL barrier for business users | Cortex Agent + Semantic View |\n",
    "| Version control | No source control for Snowflake objects | Git Repository integration |"
   ],
   "id": "cell-44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Final summary: All demo objects created\nSELECT 'Database' AS object_type, 'FREIGHT_DEMO' AS object_name, 'Isolated demo environment' AS purpose\nUNION ALL SELECT 'Schema', 'RAW', 'Simulated Oracle replication + JSON ingestion'\nUNION ALL SELECT 'Schema', 'ANALYTICS', 'The Broker Object (Golden Record)'\nUNION ALL SELECT 'Schema', 'DS_SANDBOX', 'Zero-Copy Clone for Data Science'\nUNION ALL SELECT 'Schema', 'ML', 'Model Registry, UDFs, and ML artifacts'\nUNION ALL SELECT 'Dynamic Table', 'broker_360', 'Single Source of Truth - auto-refreshing every 5 min'\nUNION ALL SELECT 'Semantic View', 'BROKER_360_SV', 'Business-friendly model for Cortex AI'\nUNION ALL SELECT 'Cortex Agent', 'BROKER_AGENT', 'Natural language broker intelligence'\nUNION ALL SELECT 'UDF', 'GET_RECOMMENDATION_SCORE()', 'ML-backed load matching for CarrierMatch app'\nUNION ALL SELECT 'Stream', 'INVOICE_JSON_STREAM', 'Tracks new JSON ingestion events'\nUNION ALL SELECT 'Task', 'SIMULATE_STREAMING_INGESTION', 'Simulates real-time Snowpipe Streaming'\nUNION ALL SELECT 'Masking Policy', 'SSN_MASK / BANK_ACCOUNT_MASK', 'PII protection via Snowflake Horizon'\nUNION ALL SELECT 'Roles', 'FREIGHT_ANALYST / FREIGHT_DATA_SCIENTIST / FREIGHT_OPS', 'RBAC governance'\nUNION ALL SELECT 'Warehouse', 'ANALYTICS_WH (Medium)', 'Production analytics (isolated)'\nUNION ALL SELECT 'Warehouse', 'DS_SANDBOX_WH (X-Small)', 'Data Science workloads (isolated)'\nUNION ALL SELECT 'Compute Pool', 'GPU_POOL (GPU_NV_S)', 'GPU compute for PyTorch training'\nUNION ALL SELECT 'API Integration', 'GIT_API_INTEGRATION', 'GitHub API access for Git Repository'\nUNION ALL SELECT 'Git Repository', 'LOADSTAR_REPO', 'Version-controlled demo artifacts from GitHub'\n"
   ],
   "id": "cell-45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snowpark (Python)",
   "language": "python",
   "name": "snowpark"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}