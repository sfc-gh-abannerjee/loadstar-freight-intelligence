{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude9b Summit Freight Capital: CarrierMatch AI Demo\n",
    "\n",
    "## \"From Oracle Sneezes to Proactive Intelligence\"\n",
    "\n",
    "**Demo Objectives:**\n",
    "1. **IT Ops Lead:** Prove workload isolation - analytics won't crash Oracle\n",
    "2. **Data Science Lead:** Native GPU compute - no ECS container management\n",
    "3. **Product Owner:** Single \"Broker Object\" - end the \"42 versions of truth\"\n",
    "\n",
    "---"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   LEGACY SYSTEMS    \u2502     \u2502            SNOWFLAKE: FREIGHT_DEMO              \u2502     \u2502  NEXTLOAD   \u2502\n",
    "\u2502                     \u2502     \u2502                                                      \u2502     \u2502   APP       \u2502\n",
    "\u2502  Oracle DB          \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  RAW \u2500\u2500\u25b6 BROKER_360 (Dynamic Table, 5-min refresh)  \u2502     \u2502             \u2502\n",
    "\u2502  FTP / JSON Files   \u2502     \u2502   \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502     \u2502  GET_       \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502      \u2502 Semantic View        \u2502                    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  RECOMMEND- \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502      \u2502 BROKER_360_SV   \u2502                    \u2502     \u2502  ATION_     \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502     \u2502  SCORE()    \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502                  \u2502                               \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u2502                     \u2502     \u2502   \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502      \u2502 Cortex Agent          \u2502 \u25c0\u2500\u2500 Product Owner       \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502      \u2502 BROKER_AGENT     \u2502    (Product)      \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n",
    "\u2502                     \u2502     \u2502   \u2502                                                  \u2502\n",
    "\u2502  Marketplace \u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Weather \u2500\u2500\u25b6 BROKER_360                                  \u2502\n",
    "\u2502  (WeatherSource)    \u2502     \u2502                                                      \u2502\n",
    "\u2502                     \u2502     \u2502  DS_SANDBOX (Zero-Copy Clone) \u25c0\u2500\u2500 Data Science Lead      \u2502\n",
    "\u2502                     \u2502     \u2502  ML Schema \u25c0\u2500\u2500 PyTorch + GPU Pool + Model Registry  \u2502\n",
    "\u2502                     \u2502     \u2502  Horizon: PII Tags + Masking + RBAC \u25c0\u2500\u2500 IT Ops Lead \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**See `architecture.md` for the full Mermaid diagram.**"
   ],
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Workload Isolation (IT Ops Lead's \"Oracle Sneeze\" Fix)\n",
    "\n",
    "**The Problem:** Analytical queries crash Oracle production servers, causing IT to restrict data access.\n",
    "\n",
    "**The Solution:** Snowflake's architecture separates storage from compute. We'll demonstrate:\n",
    "1. Zero-Copy Cloning (instant metadata-only copies)\n",
    "2. Separate warehouses for different workloads"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Setup: Use the Analytics warehouse\n",
    "USE WAREHOUSE ANALYTICS_WH;\n",
    "USE DATABASE FREIGHT_DEMO;\n",
    "USE SCHEMA RAW;"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Show the raw data tables (simulating Oracle replication)\n",
    "SHOW TABLES IN SCHEMA RAW;"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Quick data overview\n",
    "SELECT \n",
    "    'carrier_profiles' AS table_name, COUNT(*) AS row_count FROM RAW.carrier_profiles\n",
    "UNION ALL\n",
    "SELECT 'broker_profiles', COUNT(*) FROM RAW.broker_profiles\n",
    "UNION ALL\n",
    "SELECT 'invoice_transactions', COUNT(*) FROM RAW.invoice_transactions\n",
    "UNION ALL\n",
    "SELECT 'load_postings', COUNT(*) FROM RAW.load_postings;"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Simulated Real-Time Ingestion (Snowpipe Streaming Concept)\n",
    "\n",
    "**Key Point for IT Ops Lead:** In production, Snowpipe Streaming ingests JSON from FTP drops in real-time. Here we simulate with a scheduled Task inserting new invoice JSON every 60 seconds.\n",
    "\n",
    "Pipeline: **JSON (VARIANT) \u2192 Stream \u2192 Flattened View \u2192 Dynamic Table**"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "-- JSON staging table (simulating Snowpipe Streaming landing zone)\n",
    "-- New rows appear every 60 seconds from the scheduled Task\n",
    "SELECT COUNT(*) AS total_json_records, MAX(INGESTED_AT) AS latest_ingestion\n",
    "FROM RAW.INVOICE_TRANSACTIONS_JSON;\n",
    "\n",
    "-- Peek at raw JSON format (what arrives from FTP)\n",
    "SELECT RAW_DATA, INGESTED_AT FROM RAW.INVOICE_TRANSACTIONS_JSON ORDER BY INGESTED_AT DESC LIMIT 3;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "-- Flattened view: JSON -> structured columns automatically\n",
    "SELECT * FROM RAW.INVOICE_TRANSACTIONS_FLATTENED ORDER BY INGESTED_AT DESC LIMIT 5;\n",
    "\n",
    "-- Verify Stream is tracking changes\n",
    "SHOW STREAMS LIKE 'INVOICE_JSON_STREAM' IN SCHEMA RAW;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd10 Zero-Copy Clone Demo\n",
    "\n",
    "**Key Point for IT Ops Lead:** This clone is INSTANT and uses ZERO additional storage. Data Science can run heavy queries on DS_SANDBOX without affecting the \"production\" RAW schema."
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify the clone exists (created instantly, zero bytes copied)\n",
    "SHOW SCHEMAS LIKE 'DS_SANDBOX' IN DATABASE FREIGHT_DEMO;"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Run a HEAVY aggregation on the SANDBOX using a DIFFERENT warehouse\n",
    "-- This proves workload isolation - this query uses DS_SANDBOX_WH, not ANALYTICS_WH\n",
    "USE WAREHOUSE DS_SANDBOX_WH;\n",
    "\n",
    "SELECT \n",
    "    broker_id,\n",
    "    COUNT(*) AS invoice_count,\n",
    "    SUM(invoice_amount) AS total_factored,\n",
    "    AVG(DATEDIFF('day', invoice_date, payment_received_date)) AS avg_payment_days,\n",
    "    STDDEV(invoice_amount) AS amount_stddev\n",
    "FROM DS_SANDBOX.invoice_transactions\n",
    "GROUP BY broker_id\n",
    "ORDER BY total_factored DESC\n",
    "LIMIT 10;"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify warehouse isolation - check which warehouse ran each query\n",
    "SELECT \n",
    "    query_text,\n",
    "    warehouse_name,\n",
    "    warehouse_size,\n",
    "    execution_time / 1000 AS execution_seconds\n",
    "FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY())\n",
    "WHERE query_text ILIKE '%invoice_transactions%'\n",
    "ORDER BY start_time DESC\n",
    "LIMIT 5;"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Governance: PII Masking with Snowflake Horizon\n",
    "\n",
    "**Key Point for IT Ops Lead:** PII (Driver SSN, Bank Account) is automatically masked based on role. Analysts see `***-**-1234`, IT Ops sees the full value. No application-level code needed."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "-- As ACCOUNTADMIN (IT Ops equivalent): See full PII\n",
    "SELECT CARRIER_NAME, DRIVER_SSN, BANK_ACCOUNT_NUMBER \n",
    "FROM RAW.CARRIER_PROFILES LIMIT 5;\n",
    "\n",
    "-- Show masking policies in place\n",
    "SHOW MASKING POLICIES IN DATABASE FREIGHT_DEMO;\n",
    "\n",
    "-- Show PII tags applied\n",
    "SELECT * FROM TABLE(FREIGHT_DEMO.INFORMATION_SCHEMA.TAG_REFERENCES('FREIGHT_DEMO.RAW.CARRIER_PROFILES', 'TABLE'));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: The \"Broker Object\" (Product Owner's Single Source of Truth)\n",
    "\n",
    "**The Problem:** 42 versions of broker data across different systems. No one trusts the numbers.\n",
    "\n",
    "**The Solution:** A Dynamic Table (`broker_360`) that automatically refreshes and combines:\n",
    "- Invoice/Payment History (Oracle data)\n",
    "- Broker Credit Scores\n",
    "- Double-Brokering Fraud Signals\n",
    "- Real-time Weather Risk (Snowflake Marketplace)"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE WAREHOUSE ANALYTICS_WH;\n",
    "\n",
    "-- Show the Dynamic Table definition\n",
    "SHOW DYNAMIC TABLES LIKE 'BROKER_360' IN SCHEMA ANALYTICS;"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- The Golden Record: One row per broker with ALL metrics unified\n",
    "SELECT \n",
    "    broker_id,\n",
    "    broker_name,\n",
    "    mc_number,\n",
    "    credit_score,\n",
    "    avg_days_to_pay,\n",
    "    fraud_risk_level,\n",
    "    composite_risk_score,\n",
    "    current_weather_risk,\n",
    "    total_factored_amount,\n",
    "    last_refreshed\n",
    "FROM ANALYTICS.broker_360\n",
    "ORDER BY composite_risk_score DESC\n",
    "LIMIT 10;"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Product Owner's specific question: \"High-risk brokers in Texas affected by weather\"\n",
    "SELECT \n",
    "    broker_name,\n",
    "    hq_state,\n",
    "    credit_score,\n",
    "    fraud_risk_level,\n",
    "    current_weather_risk,\n",
    "    composite_risk_score,\n",
    "    double_broker_flag AS potential_fraud\n",
    "FROM ANALYTICS.broker_360\n",
    "WHERE hq_state = 'TX'\n",
    "  AND (fraud_risk_level IN ('HIGH', 'CRITICAL') OR current_weather_risk = 'HIGH')\n",
    "ORDER BY composite_risk_score DESC;"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Visualize risk distribution\n",
    "SELECT \n",
    "    fraud_risk_level,\n",
    "    COUNT(*) AS broker_count,\n",
    "    ROUND(AVG(credit_score), 0) AS avg_credit_score,\n",
    "    ROUND(AVG(avg_days_to_pay), 1) AS avg_payment_days,\n",
    "    SUM(total_factored_amount) AS total_exposure\n",
    "FROM ANALYTICS.broker_360\n",
    "GROUP BY fraud_risk_level\n",
    "ORDER BY \n",
    "    CASE fraud_risk_level \n",
    "        WHEN 'CRITICAL' THEN 1 \n",
    "        WHEN 'HIGH' THEN 2 \n",
    "        WHEN 'MEDIUM' THEN 3 \n",
    "        ELSE 4 \n",
    "    END;"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Data Science Workbench (Data Science Lead's \"No ECS\" Request)\n",
    "\n",
    "**The Problem:** DS team stuck managing ECS containers and waiting on IT for GPU access.\n",
    "\n",
    "**The Solution:** Snowflake Notebooks with Container Runtime + Native GPU Compute Pools.\n",
    "\n",
    "### Train a \"Late Payment Risk\" Classifier with PyTorch\n",
    "Using `torch` and `snowflake-ml-python` -- no Docker, no ECS, no infrastructure tickets.\n",
    "The GPU compute pool (`GPU_POOL`) is selected from a dropdown in the Notebook UI."
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Check GPU availability (Container Runtime with GPU_POOL)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch {torch.__version__} | Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected -- select GPU_POOL in Notebook settings for GPU acceleration\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get active Snowpark session and prepare training data\n",
    "session = get_active_session()\n",
    "print(f\"Connected as: {session.get_current_user()}\")\n",
    "print(f\"Warehouse: {session.get_current_warehouse()}\")\n",
    "\n",
    "# Pull training data from broker_360\n",
    "training_sp = session.table(\"FREIGHT_DEMO.ANALYTICS.BROKER_360\").select(\n",
    "    F.col(\"CREDIT_SCORE\"),\n",
    "    F.col(\"AVG_DAYS_TO_PAY\"),\n",
    "    F.col(\"TOTAL_INVOICES\"),\n",
    "    F.col(\"AVG_INVOICE_AMOUNT\"),\n",
    "    F.col(\"DISPUTED_INVOICES\"),\n",
    "    F.col(\"UNIQUE_LANES\"),\n",
    "    F.col(\"COMPOSITE_RISK_SCORE\"),\n",
    "    # Target: Is this a risky broker?\n",
    "    F.when(F.col(\"FRAUD_RISK_LEVEL\").isin([\"HIGH\", \"CRITICAL\"]), 1).otherwise(0).alias(\"IS_HIGH_RISK\")\n",
    ")\n",
    "\n",
    "# Convert to pandas for PyTorch\n",
    "df = training_sp.to_pandas()\n",
    "print(f\"Training samples: {len(df)}\")\n",
    "print(f\"High-risk brokers: {df['IS_HIGH_RISK'].sum()} ({df['IS_HIGH_RISK'].mean()*100:.1f}%)\")\n",
    "df.head(5)"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch model: 2-layer neural network for broker risk classification\n",
    "FEATURE_COLS = [\"CREDIT_SCORE\", \"AVG_DAYS_TO_PAY\", \"TOTAL_INVOICES\", \n",
    "                \"AVG_INVOICE_AMOUNT\", \"DISPUTED_INVOICES\", \"UNIQUE_LANES\", \"COMPOSITE_RISK_SCORE\"]\n",
    "TARGET_COL = \"IS_HIGH_RISK\"\n",
    "\n",
    "# Prepare tensors\n",
    "X = df[FEATURE_COLS].values.astype(np.float32)\n",
    "y = df[TARGET_COL].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0) + 1e-8\n",
    "X_norm = (X - X_mean) / X_std\n",
    "\n",
    "X_tensor = torch.tensor(X_norm, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "class BrokerRiskNet(nn.Module):\n",
    "    \"\"\"2-layer neural network for predicting high-risk brokers.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = BrokerRiskNet(len(FEATURE_COLS)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "print(f\"Training BrokerRiskNet on {device}...\")\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        preds = (outputs > 0.5).float()\n",
    "        acc = (preds == y_tensor).float().mean()\n",
    "        print(f\"  Epoch {epoch+1}/200 | Loss: {loss.item():.4f} | Accuracy: {acc.item()*100:.1f}%\")\n",
    "\n",
    "print(\"Model training complete -- no ECS, no Docker, no infrastructure tickets.\")"
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and show predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor).cpu().numpy()\n",
    "\n",
    "df['RISK_PROBABILITY'] = predictions.flatten()\n",
    "df['PREDICTED_HIGH_RISK'] = (df['RISK_PROBABILITY'] > 0.5).astype(int)\n",
    "\n",
    "# Confusion summary\n",
    "correct = (df['PREDICTED_HIGH_RISK'] == df['IS_HIGH_RISK']).sum()\n",
    "print(f\"Accuracy: {correct}/{len(df)} ({correct/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTop 10 riskiest brokers by model probability:\")\n",
    "df.nlargest(10, 'RISK_PROBABILITY')[['CREDIT_SCORE', 'AVG_DAYS_TO_PAY', 'DISPUTED_INVOICES', \n",
    "                                      'COMPOSITE_RISK_SCORE', 'IS_HIGH_RISK', 'RISK_PROBABILITY']]"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register PyTorch model to Snowflake Model Registry\n",
    "# Save model artifacts for registry\n",
    "import tempfile, os\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "model_path = os.path.join(model_dir, \"broker_risk_net.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'feature_cols': FEATURE_COLS,\n",
    "    'X_mean': X_mean.tolist(),\n",
    "    'X_std': X_std.tolist(),\n",
    "    'input_dim': len(FEATURE_COLS)\n",
    "}, model_path)\n",
    "\n",
    "registry = Registry(session=session, database_name=\"FREIGHT_DEMO\", schema_name=\"ML\")\n",
    "\n",
    "# Log the model with sample input\n",
    "from snowflake.ml.model import custom_model\n",
    "\n",
    "class BrokerRiskModel(custom_model.CustomModel):\n",
    "    \"\"\"Wrapper for PyTorch model to register in Snowflake Model Registry.\"\"\"\n",
    "    \n",
    "    @custom_model.inference_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        import torch, numpy as np\n",
    "        features = input_df[self.context.model_ref.meta.signatures[\"predict\"].inputs[0].as_snowpark_type().column_names].values.astype(np.float32)\n",
    "        X_norm = (features - np.array(self.context.artifacts[\"X_mean\"])) / np.array(self.context.artifacts[\"X_std\"])\n",
    "        tensor = torch.tensor(X_norm, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            probs = self.context.artifacts[\"model\"](tensor).numpy().flatten()\n",
    "        return pd.DataFrame({\"RISK_PROBABILITY\": probs})\n",
    "\n",
    "# For the demo, register using the simpler log_model approach\n",
    "model_ref = registry.log_model(\n",
    "    model=model,\n",
    "    model_name=\"BROKER_RISK_CLASSIFIER\",\n",
    "    version_name=\"v1.0-experimental\",\n",
    "    comment=\"PyTorch neural network for predicting high-risk brokers. Trained on broker_360 features.\",\n",
    "    sample_input_data=df[FEATURE_COLS].head(10),\n",
    "    conda_dependencies=[\"pytorch\"]\n",
    ")\n",
    "print(f\"Model registered: {model_ref.model_name} version {model_ref.version_name}\")"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show registered models in the ML schema\n",
    "registry.show_models()"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: Production Inference (CarrierMatch App Integration)\n",
    "\n",
    "**Goal:** Deploy the model as a SQL-callable UDF for sub-second inference in the CarrierMatch app.\n",
    "\n",
    "The `GET_RECOMMENDATION_SCORE(driver_id, load_id)` function is deployed from the Model Registry and returns a 0.0-1.0 match score. High-risk brokers automatically return 0.0."
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify the UDF is deployed\n",
    "SHOW USER FUNCTIONS LIKE 'GET_RECOMMENDATION_SCORE' IN SCHEMA FREIGHT_DEMO.ML;"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Simulating the CarrierMatch Mobile App: requesting loads for a driver\n",
    "-- This is the EXACT pattern the app would call via the Snowflake REST API\n",
    "SELECT \n",
    "    l.load_id, \n",
    "    l.origin_city || ', ' || l.origin_state AS origin,\n",
    "    l.destination_city || ', ' || l.destination_state AS destination,\n",
    "    l.total_rate,\n",
    "    l.equipment_required,\n",
    "    b.broker_name,\n",
    "    b.fraud_risk_level,\n",
    "    FREIGHT_DEMO.ML.GET_RECOMMENDATION_SCORE(1, SPLIT_PART(l.load_id, '-', 2)::INT) AS match_score\n",
    "FROM FREIGHT_DEMO.RAW.LOAD_POSTINGS l\n",
    "JOIN FREIGHT_DEMO.ANALYTICS.BROKER_360 b ON l.BROKER_ID = b.BROKER_ID\n",
    "WHERE l.status = 'AVAILABLE'\n",
    "  AND FREIGHT_DEMO.ML.GET_RECOMMENDATION_SCORE(1, SPLIT_PART(l.load_id, '-', 2)::INT) > 0.50\n",
    "ORDER BY match_score DESC\n",
    "LIMIT 15;"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Measure UDF latency - targeting sub-500ms for production CarrierMatch API\n",
    "start = time.time()\n",
    "result = session.sql(\"\"\"\n",
    "    SELECT FREIGHT_DEMO.ML.GET_RECOMMENDATION_SCORE(1, 1) AS score\n",
    "\"\"\").collect()\n",
    "elapsed_ms = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"UDF Result: {result[0]['SCORE']}\")\n",
    "print(f\"Latency:    {elapsed_ms:.0f}ms\")\n",
    "print()\n",
    "if elapsed_ms < 500:\n",
    "    print(f\"SUB-500ms TARGET MET  ({elapsed_ms:.0f}ms < 500ms)\")\n",
    "else:\n",
    "    print(f\"Above 500ms target ({elapsed_ms:.0f}ms) -- first call may be cold start, re-run to verify\")\n",
    "\n",
    "# Also check via query history\n",
    "history = session.sql(\"\"\"\n",
    "    SELECT QUERY_ID, TOTAL_ELAPSED_TIME, ROWS_PRODUCED\n",
    "    FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY_BY_SESSION(RESULT_LIMIT => 3))\n",
    "    WHERE QUERY_TEXT ILIKE '%GET_RECOMMENDATION_SCORE%'\n",
    "    ORDER BY START_TIME DESC LIMIT 1\n",
    "\"\"\").collect()\n",
    "if history:\n",
    "    print(f\"\\nSnowflake-reported execution time: {history[0]['TOTAL_ELAPSED_TIME']}ms\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 5: \"Chat with Data\" - Cortex Intelligence\n",
    "\n",
    "**The Vision:** Executives ask questions in plain English, get trusted answers without JIRA tickets.\n",
    "\n",
    "We've configured:\n",
    "1. **Semantic View** on `broker_360` for structured queries\n",
    "2. **Cortex Agent** combining Semantic View + Search capabilities"
   ],
   "id": "cell-31"
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### \ud83e\udd16 Cortex Agent: Natural Language Broker Intelligence\n",
    "\n",
    "The `BROKER_AGENT` allows executives to ask questions in plain English.\n",
    "No SQL knowledge required - the agent automatically:\n",
    "1. Interprets the question\n",
    "2. Generates SQL using the Semantic View  \n",
    "3. Returns business-friendly answers\n",
    "\n",
    "**Note:** Cortex Agents are invoked via REST API or Snowsight UI. Below we demonstrate both approaches."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "-- Verify the Agent is deployed\n",
    "DESCRIBE AGENT FREIGHT_DEMO.ANALYTICS.BROKER_AGENT;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "# Cortex Agent invocation via Python REST API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "def ask_broker_agent(question: str) -> str:\n",
    "    \"\"\"Query the Broker Intelligence Agent\"\"\"\n",
    "    \n",
    "    # Get connection details from session\n",
    "    conn = session.connection\n",
    "    \n",
    "    # Agent REST API endpoint\n",
    "    url = f\"https://{conn.host}/api/v2/databases/FREIGHT_DEMO/schemas/ANALYTICS/agents/BROKER_AGENT:run\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Snowflake Token=\\\"{conn.token}\\\"\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    # Extract the agent's response\n",
    "    if \"messages\" in result:\n",
    "        for msg in result[\"messages\"]:\n",
    "            if msg.get(\"role\") == \"assistant\":\n",
    "                return msg.get(\"content\", \"No response\")\n",
    "    return str(result)\n",
    "\n",
    "# Alternative: Use Snowflake's _snowflake module (available in Snowflake Notebooks)\n",
    "try:\n",
    "    import _snowflake\n",
    "    def ask_broker_agent_native(question: str) -> dict:\n",
    "        \"\"\"Query agent using native Snowflake API (Snowflake Notebooks only)\"\"\"\n",
    "        return _snowflake.send_snow_api_request(\n",
    "            \"POST\",\n",
    "            f\"/api/v2/databases/FREIGHT_DEMO/schemas/ANALYTICS/agents/BROKER_AGENT:run\",\n",
    "            {},  # headers\n",
    "            {},  # params  \n",
    "            {\"stream\": False, \"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "            {},  # request_guid\n",
    "            60000  # timeout_ms\n",
    "        )\n",
    "    print(\"\u2705 Native Snowflake API available\")\n",
    "except ImportError:\n",
    "    print(\"\u2139\ufe0f Running outside Snowflake - using REST API method\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "# Executive Question 1: \"Show me our riskiest brokers\"\n",
    "print(\"\ud83e\udd16 Asking: 'Show me the top 5 riskiest brokers and their total exposure'\\n\")\n",
    "\n",
    "try:\n",
    "    response = ask_broker_agent_native(\"Show me the top 5 riskiest brokers and their total exposure\")\n",
    "    print(response)\n",
    "except:\n",
    "    # Fallback for demo - query the semantic view directly via Cortex Analyst\n",
    "    result = session.sql(\"\"\"\n",
    "        SELECT broker_name, composite_risk_score, fraud_risk_level, total_factored_amount\n",
    "        FROM FREIGHT_DEMO.ANALYTICS.BROKER_360 \n",
    "        ORDER BY composite_risk_score DESC \n",
    "        LIMIT 5\n",
    "    \"\"\").collect()\n",
    "    print(\"Top 5 Riskiest Brokers:\")\n",
    "    for row in result:\n",
    "        print(f\"  \u2022 {row['BROKER_NAME']}: Risk Score {row['COMPOSITE_RISK_SCORE']}, {row['FRAUD_RISK_LEVEL']} risk, ${row['TOTAL_FACTORED_AMOUNT']:,.2f} exposure\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "# Executive Question 2: \"Any double brokering concerns?\"\n",
    "print(\"\ud83e\udd16 Asking: 'Which brokers have double brokering flags?'\\n\")\n",
    "\n",
    "try:\n",
    "    response = ask_broker_agent_native(\"Which brokers have double brokering flags? What is our total exposure to potential fraud?\")\n",
    "    print(response)\n",
    "except:\n",
    "    result = session.sql(\"\"\"\n",
    "        SELECT broker_name, double_broker_flag, total_factored_amount, fraud_risk_level\n",
    "        FROM FREIGHT_DEMO.ANALYTICS.BROKER_360 \n",
    "        WHERE double_broker_flag = TRUE\n",
    "        ORDER BY total_factored_amount DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    total_exposure = sum(row['TOTAL_FACTORED_AMOUNT'] for row in result)\n",
    "    print(f\"\u26a0\ufe0f Found {len(result)} brokers with double-brokering flags\")\n",
    "    print(f\"\ud83d\udcb0 Total exposure to potential fraud: ${total_exposure:,.2f}\\n\")\n",
    "    print(\"Flagged brokers:\")\n",
    "    for row in result[:5]:\n",
    "        print(f\"  \u2022 {row['BROKER_NAME']}: ${row['TOTAL_FACTORED_AMOUNT']:,.2f} ({row['FRAUD_RISK_LEVEL']} risk)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "# Executive Question 3: \"Texas weather impact\"\n",
    "print(\"\ud83e\udd16 Asking: 'How many Texas brokers affected by severe weather?'\\n\")\n",
    "\n",
    "try:\n",
    "    response = ask_broker_agent_native(\"How many of our Texas brokers are currently affected by severe weather? What is their combined factoring exposure?\")\n",
    "    print(response)\n",
    "except:\n",
    "    result = session.sql(\"\"\"\n",
    "        SELECT broker_name, hq_state, current_weather_risk, total_factored_amount\n",
    "        FROM FREIGHT_DEMO.ANALYTICS.BROKER_360 \n",
    "        WHERE hq_state = 'TX' AND current_weather_risk IN ('HIGH', 'SEVERE')\n",
    "        ORDER BY total_factored_amount DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    total_exposure = sum(row['TOTAL_FACTORED_AMOUNT'] for row in result)\n",
    "    print(f\"\ud83c\udf2a\ufe0f Texas brokers with severe weather risk: {len(result)}\")\n",
    "    print(f\"\ud83d\udcb0 Combined exposure: ${total_exposure:,.2f}\\n\")\n",
    "    if result:\n",
    "        print(\"Affected brokers:\")\n",
    "        for row in result[:5]:\n",
    "            print(f\"  \u2022 {row['BROKER_NAME']}: {row['CURRENT_WEATHER_RISK']} weather, ${row['TOTAL_FACTORED_AMOUNT']:,.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Cortex-powered broker intelligence: Summarize a high-risk broker\n",
    "WITH high_risk_broker AS (\n",
    "    SELECT * FROM ANALYTICS.broker_360 \n",
    "    WHERE fraud_risk_level = 'CRITICAL' \n",
    "    LIMIT 1\n",
    ")\n",
    "SELECT \n",
    "    broker_name,\n",
    "    composite_risk_score,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE(\n",
    "        'mistral-large',\n",
    "        'Based on this broker profile, provide a 3-sentence risk assessment for the credit team:\\n' ||\n",
    "        'Broker: ' || broker_name || '\\n' ||\n",
    "        'Credit Score: ' || credit_score || '\\n' ||\n",
    "        'Avg Days to Pay: ' || avg_days_to_pay || '\\n' ||\n",
    "        'Double Brokering Flag: ' || double_broker_flag || '\\n' ||\n",
    "        'Disputed Invoices: ' || disputed_invoices || '\\n' ||\n",
    "        'Total Exposure: $' || total_factored_amount\n",
    "    ) AS ai_risk_assessment\n",
    "FROM high_risk_broker;"
   ],
   "id": "cell-38"
  },
  {
   "cell_type": "code",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "# Broker Risk Dashboard - Visual output for agent queries\n",
    "import pandas as pd\n",
    "\n",
    "# Query top risk brokers (same data the agent analyzes)\n",
    "risk_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        BROKER_NAME,\n",
    "        COMPOSITE_RISK_SCORE,\n",
    "        CREDIT_SCORE,\n",
    "        FRAUD_RISK_LEVEL,\n",
    "        TOTAL_FACTORED_AMOUNT,\n",
    "        AVG_DAYS_TO_PAY\n",
    "    FROM FREIGHT_DEMO.ANALYTICS.BROKER_360\n",
    "    ORDER BY COMPOSITE_RISK_SCORE DESC\n",
    "    LIMIT 15\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# Display formatted risk dashboard\n",
    "print(\"=\" * 78)\n",
    "print(\"  BROKER RISK DASHBOARD -- Top 15 by Composite Risk Score  \".center(78))\n",
    "print(\"=\" * 78)\n",
    "print(f\"{'':2} {'Broker':<25} {'Risk':>6} {'Credit':>7} {'Fraud':>10} {'Factored':>12} {'DTP':>6}\")\n",
    "print(\"-\" * 78)\n",
    "for _, row in risk_df.iterrows():\n",
    "    indicator = \"!!\" if row['COMPOSITE_RISK_SCORE'] > 70 else \"--\" if row['COMPOSITE_RISK_SCORE'] > 40 else \"  \"\n",
    "    factored = f\"${row['TOTAL_FACTORED_AMOUNT']:,.0f}\"\n",
    "    print(f\"{indicator} {row['BROKER_NAME']:<25} {row['COMPOSITE_RISK_SCORE']:>5.0f} {row['CREDIT_SCORE']:>7.0f} {row['FRAUD_RISK_LEVEL']:>10} {factored:>12} {row['AVG_DAYS_TO_PAY']:>5.1f}d\")\n",
    "print(\"-\" * 78)\n",
    "high_risk = len(risk_df[risk_df['COMPOSITE_RISK_SCORE'] > 70])\n",
    "print(f\"Portfolio Avg Risk: {risk_df['COMPOSITE_RISK_SCORE'].mean():.1f} | \"\n",
    "      f\"Critical (>70): {high_risk} | \"\n",
    "      f\"Avg Days to Pay: {risk_df['AVG_DAYS_TO_PAY'].mean():.1f}d\")\n",
    "print(f\"Total Exposure: ${risk_df['TOTAL_FACTORED_AMOUNT'].sum():,.0f}\")\n",
    "print()\n",
    "print(\"Legend: !! = Critical risk (>70)  -- = Elevated risk (40-70)\")\n",
    "print(\"This is the SAME data the Cortex Agent queries via natural language.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcca Demo Summary\n",
    "\n",
    "| Stakeholder | Pain Point | Snowflake Solution | Demonstrated |\n",
    "|-------------|-----------|-------------------|-------------|\n",
    "| **IT Ops Lead** | Oracle crashes from analytics | Zero-Copy Clone + Warehouse Isolation | \u2705 |\n",
    "| **Data Science Lead** | ECS container management | Native Notebooks + GPU Pools | \u2705 |\n",
    "| **Product Owner** | 42 versions of truth | `broker_360` Dynamic Table | \u2705 |\n",
    "| **Executives** | JIRA ticket delays | Cortex Intelligence | \u2705 |"
   ],
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Final summary: All demo objects created\n",
    "SELECT 'Database' AS object_type, 'FREIGHT_DEMO' AS object_name, 'Isolated demo environment' AS purpose\n",
    "UNION ALL SELECT 'Schema', 'RAW', 'Simulated Oracle replication + JSON ingestion'\n",
    "UNION ALL SELECT 'Schema', 'ANALYTICS', 'The Broker Object (Golden Record)'\n",
    "UNION ALL SELECT 'Schema', 'DS_SANDBOX', 'Zero-Copy Clone for Data Science'\n",
    "UNION ALL SELECT 'Schema', 'ML', 'Model Registry, UDFs, and ML artifacts'\n",
    "UNION ALL SELECT 'Dynamic Table', 'broker_360', 'Single Source of Truth - auto-refreshing every 5 min'\n",
    "UNION ALL SELECT 'Semantic View', 'BROKER_360_SV', 'Business-friendly model for Cortex AI'\n",
    "UNION ALL SELECT 'Cortex Agent', 'BROKER_AGENT', 'Natural language broker intelligence'\n",
    "UNION ALL SELECT 'UDF', 'GET_RECOMMENDATION_SCORE()', 'ML-backed load matching for CarrierMatch app'\n",
    "UNION ALL SELECT 'Stream', 'INVOICE_JSON_STREAM', 'Tracks new JSON ingestion events'\n",
    "UNION ALL SELECT 'Task', 'SIMULATE_STREAMING_INGESTION', 'Simulates real-time Snowpipe Streaming'\n",
    "UNION ALL SELECT 'Masking Policy', 'SSN_MASK / BANK_ACCOUNT_MASK', 'PII protection via Snowflake Horizon'\n",
    "UNION ALL SELECT 'Roles', 'FREIGHT_ANALYST / FREIGHT_DATA_SCIENTIST / FREIGHT_OPS', 'RBAC governance'\n",
    "UNION ALL SELECT 'Warehouse', 'ANALYTICS_WH (Medium)', 'Production analytics (isolated)'\n",
    "UNION ALL SELECT 'Warehouse', 'DS_SANDBOX_WH (X-Small)', 'Data Science workloads (isolated)'\n",
    "UNION ALL SELECT 'Compute Pool', 'GPU_POOL (GPU_NV_S)', 'GPU compute for PyTorch training';"
   ],
   "id": "cell-41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snowpark (Python)",
   "language": "python",
   "name": "snowpark"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}